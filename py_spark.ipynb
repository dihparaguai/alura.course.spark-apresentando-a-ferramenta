{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4092b8d",
   "metadata": {},
   "source": [
    "## Importações e inicialização do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a417b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# coloca as variáveis de ambiente necessárias para o PySpark no windows\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-8.0.452.9-hotspot\"\n",
    "os.environ['SPARK_HOME'] = r\"C:\\spark\\spark-3.5.6-bin-hadoop3\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af419d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark # busca o PySpark no sistema\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381108af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe que inicia uma sessão do Spark para usar o Spark SQL\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicia uma sessão do Spark, que é o ponto de entrada para usar o Spark SQL\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"Iniciando\").config(\"spark.hadoop.io.native.lib\", \"false\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d7593",
   "metadata": {},
   "source": [
    "## Exemplos de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando tuplas\n",
    "data = [('zeca', '35'), ('eva', '29')]\n",
    "col_names = ['nome', 'idade']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando dicionários\n",
    "data_dict = [{'nome': 'zeca', 'idade': '35'}, {'nome': 'eva', 'idade': '29'}]\n",
    "\n",
    "df = spark.createDataFrame(data_dict)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d643b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6664f8",
   "metadata": {},
   "source": [
    "## Extrair conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = os.path.abspath(r'.\\zip_files')\n",
    "data_path = os.path.abspath(r'.\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c89840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile # biblioteca nativa do Python para manipular arquivos zip\n",
    "\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'empresas.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'estabelecimentos.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'socios.zip'), 'r').extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna uma lista com os caminhos dos arquivos de um diretório\n",
    "def get_files_from_directory(directory):\n",
    "    files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "    ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lê todos os arquivos CSV do diretório especificado e cria um DataFrame\n",
    "df_emps = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'empresas')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_emps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'estabelecimentos')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_estabs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc214fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'socios')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_socios.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames\n",
    "df_emps.show(3)\n",
    "df_estabs.show(3)\n",
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para um DataFrame do Pandas\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362516c",
   "metadata": {},
   "source": [
    "## Renomear colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define os nomes das colunas para os DataFrames\n",
    "\n",
    "emps_col_names = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']\n",
    "\n",
    "estabs_col_names = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']\n",
    "\n",
    "socios_col_names = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41561930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe os nomes das colunas dos DataFrames em formato enumerado para mapeamento das colunas quando renomear\n",
    "for i, col_name in enumerate(emps_col_names):\n",
    "    print([i, col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para renomear as colunas de um DataFrame utilizando o mapeamento de nomes de colunas\n",
    "def rename_columns(df, col_names):\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        df = df.withColumnRenamed(f'_c{i}', col_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renomeia as colunas dos DataFrames\n",
    "df_emps = rename_columns(df_emps, emps_col_names)\n",
    "df_estabs = rename_columns(df_estabs, estabs_col_names)\n",
    "df_socios = rename_columns(df_socios, socios_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames com os nomes das colunas renomeados\n",
    "print(df_emps.columns)\n",
    "print(df_estabs.columns)\n",
    "print(df_socios.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9656cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para DataFrame do Pandas com o nomes das colunas renomeados\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6dafa2",
   "metadata": {},
   "source": [
    "# Converter para coluna de dinheiro para tipo Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostra os nomes das colunas e seus tipos de dados\n",
    "df_emps.printSchema()\n",
    "df_socios.printSchema()\n",
    "df_estabs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bba774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe DoubleType para converter o tipo de dado para Double\n",
    "# importa a classe functions do para manipulação de colunas\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as fx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emps.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ec566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitui a vírgula por ponto na coluna 'capital_social_da_empresa' para que o PySpark reconheça como número decimal\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', fx.regexp_replace('capital_social_da_empresa', ',', '.'))\n",
    "df_emps.select('capital_social_da_empresa').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e18392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte a coluna 'capital_social_da_empresa' para o tipo Double\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', df_emps['capital_social_da_empresa'].cast(DoubleType()))\n",
    "df_emps.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36794aef",
   "metadata": {},
   "source": [
    "# Converter coluna para o tipo Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe StringType para converter o tipo de dado para String\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a coluna estava com tipo de dado Integer, mas foi convertida para String para que seja possivel passar to_date\n",
    "df_socios = df_socios.withColumn('data_de_entrada_sociedade', df_socios['data_de_entrada_sociedade'].cast(StringType()))\n",
    "df_socios.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios.select('data_de_entrada_sociedade').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para converter para o tipo Date é necessario converter para String primeiro, depois converte para Date usando o mesmo formato de data na coluna original\n",
    "df_socios = df_socios.withColumn('data_de_entrada_sociedade', fx.to_date(df_socios['data_de_entrada_sociedade'], 'yyyyMMdd'))\n",
    "\n",
    "df_socios.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209df80a",
   "metadata": {},
   "source": [
    "# Converter multiplas colunas em tipo Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8097c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1728a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplica a conversão de data para várias colunas do DataFrame de estabelecimentos\n",
    "df_estabs = df_estabs\\\n",
    "    .withColumn('data_situacao_cadastral', fx.to_date(df_estabs.data_situacao_cadastral\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\\\n",
    "    .withColumn('data_de_inicio_atividade', fx.to_date(df_estabs.data_de_inicio_atividade\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\\\n",
    "    .withColumn('data_da_situacao_especial', fx.to_date(df_estabs.data_da_situacao_especial\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\n",
    "\n",
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
