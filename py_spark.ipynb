{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4092b8d",
   "metadata": {},
   "source": [
    "## Importações e inicialização do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a417b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# coloca as variáveis de ambiente necessárias para o PySpark no windows\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-8.0.452.9-hotspot\"\n",
    "os.environ['SPARK_HOME'] = r\"C:\\spark\\spark-3.5.6-bin-hadoop3\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af419d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark # busca o PySpark no sistema\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381108af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe que inicia uma sessão do Spark para usar o Spark SQL\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicia uma sessão do Spark, que é o ponto de entrada para usar o Spark SQL\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"Iniciando\").config(\"spark.hadoop.io.native.lib\", \"false\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d7593",
   "metadata": {},
   "source": [
    "## Exemplos de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando tuplas\n",
    "data = [('zeca', '35'), ('eva', '29')]\n",
    "col_names = ['nome', 'idade']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando dicionários\n",
    "data_dict = [{'nome': 'zeca', 'idade': '35'}, {'nome': 'eva', 'idade': '29'}]\n",
    "\n",
    "df = spark.createDataFrame(data_dict)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d643b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6664f8",
   "metadata": {},
   "source": [
    "## Extrair conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = os.path.abspath(r'.\\zip_files')\n",
    "data_path = os.path.abspath(r'.\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c89840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile # biblioteca nativa do Python para manipular arquivos zip\n",
    "\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'empresas.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'estabelecimentos.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'socios.zip'), 'r').extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna uma lista com os caminhos dos arquivos de um diretório\n",
    "def get_files_from_directory(directory):\n",
    "    files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "    ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lê todos os arquivos CSV do diretório especificado e cria um DataFrame\n",
    "df_emps = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'empresas')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_emps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'estabelecimentos')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_estabs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc214fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'socios')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_socios.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames\n",
    "df_emps.show(3)\n",
    "df_estabs.show(3)\n",
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para um DataFrame do Pandas\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362516c",
   "metadata": {},
   "source": [
    "## Renomear colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define os nomes das colunas para os DataFrames\n",
    "\n",
    "emps_col_names = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']\n",
    "\n",
    "estabs_col_names = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']\n",
    "\n",
    "socios_col_names = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41561930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe os nomes das colunas dos DataFrames em formato enumerado para mapeamento das colunas quando renomear\n",
    "for i, col_name in enumerate(emps_col_names):\n",
    "    print([i, col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para renomear as colunas de um DataFrame utilizando o mapeamento de nomes de colunas\n",
    "def rename_columns(df, col_names):\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        df = df.withColumnRenamed(f'_c{i}', col_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renomeia as colunas dos DataFrames\n",
    "df_emps = rename_columns(df_emps, emps_col_names)\n",
    "df_estabs = rename_columns(df_estabs, estabs_col_names)\n",
    "df_socios = rename_columns(df_socios, socios_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames com os nomes das colunas renomeados\n",
    "print(df_emps.columns)\n",
    "print(df_estabs.columns)\n",
    "print(df_socios.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9656cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para DataFrame do Pandas com o nomes das colunas renomeados\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6dafa2",
   "metadata": {},
   "source": [
    "## Converter para coluna de dinheiro para tipo Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostra os nomes das colunas e seus tipos de dados\n",
    "df_emps.printSchema()\n",
    "df_socios.printSchema()\n",
    "df_estabs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bba774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe DoubleType para converter o tipo de dado para Double\n",
    "# importa a classe functions do para manipulação de colunas\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as fx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emps.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ec566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitui a vírgula por ponto na coluna 'capital_social_da_empresa' para que o PySpark reconheça como número decimal\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', fx.regexp_replace('capital_social_da_empresa', ',', '.'))\n",
    "df_emps.select('capital_social_da_empresa').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e18392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte a coluna 'capital_social_da_empresa' para o tipo Double\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', df_emps['capital_social_da_empresa'].cast(DoubleType()))\n",
    "df_emps.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36794aef",
   "metadata": {},
   "source": [
    "## Converter coluna para o tipo Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe StringType para converter o tipo de dado para String\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a coluna estava com tipo de dado Integer, mas foi convertida para String para que seja possivel passar to_date\n",
    "df_socios = df_socios.withColumn('data_de_entrada_sociedade', df_socios['data_de_entrada_sociedade'].cast(StringType()))\n",
    "df_socios.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios.select('data_de_entrada_sociedade').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para converter para o tipo Date é necessario converter para String primeiro, depois converte para Date usando o mesmo formato de data na coluna original\n",
    "df_socios = df_socios.withColumn('data_de_entrada_sociedade', fx.to_date(df_socios['data_de_entrada_sociedade'], 'yyyyMMdd'))\n",
    "\n",
    "df_socios.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209df80a",
   "metadata": {},
   "source": [
    "## Converter multiplas colunas em tipo Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8097c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1728a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplica a conversão de data para várias colunas do DataFrame de estabelecimentos\n",
    "df_estabs = df_estabs\\\n",
    "    .withColumn('data_situacao_cadastral', fx.to_date(df_estabs.data_situacao_cadastral\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\\\n",
    "    .withColumn('data_de_inicio_atividade', fx.to_date(df_estabs.data_de_inicio_atividade\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\\\n",
    "    .withColumn('data_da_situacao_especial', fx.to_date(df_estabs.data_da_situacao_especial\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\n",
    "\n",
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18e0fd",
   "metadata": {},
   "source": [
    "## Extrair ano de uma coluna de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assim como no SQL, podemos aplicar funções de transformação nas colunas\n",
    "df_socios\\\n",
    "    .select(\n",
    "        'nome_do_socio_ou_razao_social',\n",
    "        fx.year('data_de_entrada_sociedade').alias('ano_entrada_sociedade'), # extrai o ano da data e renomeia a coluna\n",
    "    )\\\n",
    "    .show(5, truncate=False) # 'truncate=False' para mostrar o texto completo das colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cf5c9",
   "metadata": {},
   "source": [
    "### Exercício - Extrair parte de string de uma coluna:\n",
    "- Criar coluna com ultimo sobrenome e primeiro nome, separados por virgula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d21b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('GISELLE PAULA GUIMARAES CASTRO', 15),\n",
    "    ('ELAINE GARCIA DE OLIVEIRA', 22),\n",
    "    ('JOAO CARLOS ABNER DE LOURDES', 43),\n",
    "    ('MARTA ZELI FERREIRA', 24),\n",
    "    ('LAUDENETE WIGGERS ROEDER', 51)\n",
    "]\n",
    "col_names = ['nome', 'idade']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma coluna com o último sobrenome e o primeiro nome, separados por vírgula\n",
    "df\\\n",
    "    .select(\n",
    "        'nome',\n",
    "        fx.concat_ws( # concatena as colunas com um separador\n",
    "            ', ', \n",
    "            fx.substring_index('nome', ' ', -1), # pega o ultimo sobrenome\n",
    "            fx.substring_index('nome', ' ', 1) # pega o primeiro nome\n",
    "        ).alias('ident'), \n",
    "        'idade'\n",
    "    )\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10164a9a",
   "metadata": {},
   "source": [
    "## Diferença entre NaN, None, Null para o Pandas e o PySpark\n",
    "- para o PySpark, NaN aparece apenas para o tipo Float, Null aparece para todos os tipos de dados\n",
    "- para o Pandas, NaN aparece para todos os tipos de dados exceto para o tipo String, que aparece como None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (float('nan'), None, None, None),\n",
    "    (float('nan'), None, None, None),\n",
    "    (float('nan'), None, None, None),\n",
    "]\n",
    "\n",
    "# determina o nome e o tipo de cada coluna do DataFrame\n",
    "schema = \\\n",
    "    StructType([\n",
    "        StructField(name=\"float_nan\", dataType=FloatType(), nullable=True),\n",
    "        StructField(name=\"float_none\", dataType=FloatType(), nullable=True),\n",
    "        StructField(name=\"integer_none\", dataType=IntegerType(), nullable=True),\n",
    "        StructField(name=\"string_none\", dataType=StringType(), nullable=True),\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "# para o PySpark, NaN aparece apenas para o tipo Float, Null aparece para todos os tipos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()\n",
    "\n",
    "# para o Pandas, NaN aparece para todos os tipos de dados exceto para o tipo String, que aparece como None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d24f1",
   "metadata": {},
   "source": [
    "## Substituir valores Null de tipos diferentes\n",
    "- ao preencher os nulls com 0 (int), o PySpark troca apenas os valores da coluna com tipo numérico\n",
    "- ao preencher os nulls com '0' (str), o PySpark troca apenas os valores da coluna com tipo String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as colunas 'pais' e 'nome_do_representante' possui valores Null e são de tipos diferentes\n",
    "\n",
    "df_socios.select('pais', 'nome_do_representante').show(5, truncate=False)\n",
    "\n",
    "df_socios.select('pais', 'nome_do_representante').printSchema()\n",
    "\n",
    "df_socios.select('pais', 'nome_do_representante').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contagem de valores Null para colunas específicas\n",
    "df_socios\\\n",
    "    .select(\n",
    "    \n",
    "    fx.count( # conta o número de linhas quando a condição de ser Null é atendida\n",
    "        fx.when(\n",
    "            fx.isnull(col='pais'), 1\n",
    "        )\n",
    "    ).alias('pais_null_count'),\n",
    "    \n",
    "    fx.count(\n",
    "        fx.when(\n",
    "            fx.isnull(col='nome_do_representante'), 1\n",
    "        )\n",
    "    ).alias('nome_do_representante_null_count')\n",
    "    \n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562738fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contagem de valores Null em todas as colunas do DataFrame usando lista\n",
    "df_socios\\\n",
    "    .select(\n",
    "    [\n",
    "        fx.count( # conta o número de linhas quando a condição de ser Null é atendida para cada coluna\n",
    "            fx.when(\n",
    "                fx.isnull(col_name), 1\n",
    "            )\n",
    "        )\\\n",
    "        .alias(col_name[:10]) \n",
    "        \n",
    "        for col_name in df_socios.columns\n",
    "    ]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ao preencher os nulls com 0 (int), o PySpark troca apenas os valores da coluna com tipo numérico\n",
    "df_socios.na.fill(0).select('pais', 'nome_do_representante').show(5)\n",
    "\n",
    "# ao preencher os nulls com '0' (str), o PySpark troca apenas os valores da coluna com tipo String\n",
    "df_socios.na.fill('0').select('pais', 'nome_do_representante').show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4cd82",
   "metadata": {},
   "source": [
    "## Ordenar colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios\\\n",
    "    .select(\n",
    "        'nome_do_socio_ou_razao_social', 'data_de_entrada_sociedade'\n",
    "    )\\\n",
    "    .orderBy(\n",
    "        'data_de_entrada_sociedade', ascending=True\n",
    "    )\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atribui uma lista de colunas para o método orderBy, permitindo ordenar por múltiplas colunas\n",
    "df_socios\\\n",
    "    .select(\n",
    "        'nome_do_socio_ou_razao_social', 'data_de_entrada_sociedade'\n",
    "    )\\\n",
    "    .orderBy(\n",
    "        ['data_de_entrada_sociedade', 'nome_do_socio_ou_razao_social'], \n",
    "        ascending=[True, True]\n",
    "    )\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22535c86",
   "metadata": {},
   "source": [
    "### Exercício - Ordenar colunas:\n",
    "- Ordenar o ano e mês dos alunos mais novos para os mais velhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('CARMINA RABELO', 4, 2010), \n",
    "    ('HERONDINA PEREIRA', 6, 2009), \n",
    "    ('IRANI DOS SANTOS', 12, 2010), \n",
    "    ('JOAO BOSCO DA FONSECA', 3, 2009), \n",
    "    ('CARLITO SOUZA', 1, 2010), \n",
    "    ('WALTER DIAS', 9, 2009), \n",
    "    ('BRENO VENTUROSO', 1, 2009), \n",
    "    ('ADELINA TEIXEIRA', 5, 2009), \n",
    "    ('ELIO SILVA', 7, 2010), \n",
    "    ('DENIS FONSECA', 6, 2010)\n",
    "]\n",
    "col_names = ['nome', 'mes', 'ano']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordena por ano e mês, ambos em ordem decrescente, para que os meses mais recentes apareçam primeiro\n",
    "df\\\n",
    "    .orderBy(['ano', 'mes'], ascending=[False, False])\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca77bd",
   "metadata": {},
   "source": [
    "## Filtrar dados\n",
    "- Os métodos `.where` e `.filter` são equivalentes, e filtram da mesma forma os DataFrames\n",
    "- O método `.like` permite filtrar por parte da string utilizando `%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra os dados utilizando o método where\n",
    "df_emps\\\n",
    "    .select(df_emps.razao_social_nome_empresarial, df_emps.capital_social_da_empresa)\\\n",
    "    .where(df_emps.capital_social_da_empresa == 50)\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c405b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra os dados utilizando o método filter\n",
    "df_socios\\\n",
    "    .select(df_socios.nome_do_socio_ou_razao_social)\\\n",
    "    .filter(df_socios.nome_do_socio_ou_razao_social.startswith('DIEGO'))\\\n",
    "    .filter(df_socios.nome_do_socio_ou_razao_social.endswith('CARVALHO'))\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra os dados utilizando a função upper para tornar a busca case-insensitive\n",
    "df_emps\\\n",
    "    .select(df_emps.razao_social_nome_empresarial)\\\n",
    "    .where(fx.upper(df_emps.razao_social_nome_empresarial).like('%RESTAURANTE%'))\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664ad25",
   "metadata": {},
   "source": [
    "### Exercício 1 - Filtrar dados\n",
    "- Realizar a seleção de apenas alunos nascidos no primeiro semestre de 2009 usado os métodos where e filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74916400",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('CARMINA RABELO', 4, 2010), \n",
    "    ('HERONDINA PEREIRA', 6, 2009), \n",
    "    ('IRANI DOS SANTOS', 12, 2010), \n",
    "    ('JOAO BOSCO DA FONSECA', 3, 2009), \n",
    "    ('CARLITO SOUZA', 1, 2010), \n",
    "    ('WALTER DIAS', 9, 2009), \n",
    "    ('BRENO VENTUROSO', 1, 2009), \n",
    "    ('ADELINA TEIXEIRA', 5, 2009), \n",
    "    ('ELIO SILVA', 7, 2010), \n",
    "    ('DENIS FONSECA', 6, 2010)\n",
    "]\n",
    "col_names = ['nome', 'mes', 'ano']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=col_names)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fafcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todas as formas abaixo são equivalentes e retornam o mesmo resultado\n",
    "\n",
    "df\\\n",
    "    .where(\"ano = 2009\")\\\n",
    "    .filter(df.mes < 7)\\\n",
    "    .show(truncate=False)\n",
    "\n",
    "df\\\n",
    "    .where(df.ano == 2009)\\\n",
    "    .filter(\"mes < 7\")\\\n",
    "    .show(truncate=False)\n",
    "\n",
    "df\\\n",
    "    .filter(\"ano == 2009\")\\\n",
    "    .where(\"mes < 7\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb204b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a estrutura abaixo é considerada como boa prática, pois é mais facil e ser validada pela IDE\n",
    "df\\\n",
    "    .filter((df.ano == 2009) & (df.mes < 7))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59364ccf",
   "metadata": {},
   "source": [
    "### Exercício 2 - Filtrar dados\n",
    "- Selecionar apenas os alunos que têm seus nomes iniciados com a letra “C” usando o método like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('CARMINA RABELO', 4, 2010), \n",
    "    ('HERONDINA PEREIRA', 6, 2009), \n",
    "    ('IRANI DOS SANTOS', 12, 2010), \n",
    "    ('JOAO BOSCO DA FONSECA', 3, 2009), \n",
    "    ('CARLITO SOUZA', 1, 2010), \n",
    "    ('WALTER DIAS', 9, 2009), \n",
    "    ('BRENO VENTUROSO', 1, 2009), \n",
    "    ('ADELINA TEIXEIRA', 5, 2009), \n",
    "    ('ELIO SILVA', 7, 2010), \n",
    "    ('DENIS FONSECA', 6, 2010)\n",
    "]\n",
    "col_names = ['nome', 'mes', 'ano']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\\\n",
    "    .filter(df.nome.like('C%'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870b43d",
   "metadata": {},
   "source": [
    "### Exercício 3 - Filtrar dados\n",
    "- Aplicar filtro com case-insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('DIEGO PARAGUAI', 0), \n",
    "    ('diego paraguai', 1), \n",
    "    ('Diego Paraguai', 2), \n",
    "    ('DiegO ParaguaI', 3), \n",
    "    ('DieGo PaRaGuai', 4)\n",
    "]\n",
    "col_names = ['meu_nome', 'index']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65905c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\\\n",
    "    .where(fx.upper(df.meu_nome) == 'DIEGO PARAGUAI')\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6a4d5",
   "metadata": {},
   "source": [
    "## Sumarizar / Agrupar / Agregar dados\n",
    "- Sumarizar = resumir dados (estatística)\n",
    "- Agrupar = juntar linhas em comum de uma coluna\n",
    "- Agregar = aplicar função para cada linha agrupada (soma, média etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupa pela contagem de 'ano_de_entrada_sociedade' segmentado por 'ano_de_entrada_sociedade'\n",
    "df_socios\\\n",
    "    .select(fx.year(df_socios.data_de_entrada_sociedade).alias('ano_de_entrada_sociedade'))\\\n",
    "    .where('ano_de_entrada_sociedade >= 2010')\\\n",
    "    .groupBy('ano_de_entrada_sociedade')\\\n",
    "    .count()\\\n",
    "    .orderBy('ano_de_entrada_sociedade', ascending=True)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d77907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupa pela media de 'capital_social_da_empresa' e a contagem de 'cnpj_basico' segmentado por 'porte_da_empresa'\n",
    "# '.agg()' é uma forma de realizar vários agrupamentos de uma única vez\n",
    "df_emps\\\n",
    "    .select(df_emps.cnpj_basico, df_emps.porte_da_empresa, df_emps.capital_social_da_empresa)\\\n",
    "    .groupBy(df_emps.porte_da_empresa)\\\n",
    "    .agg(\n",
    "        fx.avg(df_emps.capital_social_da_empresa),\n",
    "        fx.count(df_emps.cnpj_basico)\n",
    "    )\\\n",
    "    .orderBy(df_emps.porte_da_empresa, ascending=True)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0135c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostra as estatisticas gerais (parecido com o método 'describe()' do Pandas)\n",
    "df_emps\\\n",
    "    .select(df_emps.capital_social_da_empresa)\\\n",
    "    .summary()\\\n",
    "    .show()\n",
    "\n",
    "df_socios\\\n",
    "    .select(df_socios.faixa_etaria)\\\n",
    "    .summary('count', 'mean')\\\n",
    "    .show()\n",
    "\n",
    "# count, mean, stddev, min, 25%, 50%, 75%, max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a276e",
   "metadata": {},
   "source": [
    "### Exercício - Sumarizar / Agrupar / Agregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa36c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('CARLOS', 'MATEMÁTICA', 7), \n",
    "    ('IVO', 'MATEMÁTICA', 9), \n",
    "    ('MÁRCIA', 'MATEMÁTICA', 8), \n",
    "    ('LEILA', 'MATEMÁTICA', 9), \n",
    "    ('BRENO', 'MATEMÁTICA', 7), \n",
    "    ('LETÍCIA', 'MATEMÁTICA', 8), \n",
    "    ('CARLOS', 'FÍSICA', 2), \n",
    "    ('IVO', 'FÍSICA', 8), \n",
    "    ('MÁRCIA', 'FÍSICA', 10), \n",
    "    ('LEILA', 'FÍSICA', 9), \n",
    "    ('BRENO', 'FÍSICA', 1), \n",
    "    ('LETÍCIA', 'FÍSICA', 6), \n",
    "    ('CARLOS', 'QUÍMICA', 10), \n",
    "    ('IVO', 'QUÍMICA', 8), \n",
    "    ('MÁRCIA', 'QUÍMICA', 1), \n",
    "    ('LEILA', 'QUÍMICA', 10), \n",
    "    ('BRENO', 'QUÍMICA', 7), \n",
    "    ('LETÍCIA', 'QUÍMICA', 9)\n",
    "]\n",
    "col_names = ['nome', 'materia', 'nota']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff337c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma coluna com 'withColumns', com valores baseado no teste feito na coluna 'nota' através do método 'when()' e 'otherwise()'\n",
    "df = df.withColumn('status', fx.when(df.nota >= 7, \"APROVADO\").otherwise(\"REPROVADO\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descreve a distribuicao dos dados de nota\n",
    "df\\\n",
    "    .select('nota')\\\n",
    "    .summary('min', '25%', '50%', '75%', 'max')\\\n",
    "    .show()\n",
    "\n",
    "# agrupa pela contagem de 'status' segmentado por 'status'\n",
    "df\\\n",
    "    .groupBy('status')\\\n",
    "    .count()\\\n",
    "    .orderBy('status', ascending=True)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8769df",
   "metadata": {},
   "source": [
    "## Juntar DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da46a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_emps.columns)\n",
    "print(df_estabs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6629706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junta internamente os DataFrames através da coluna 'cnpj_basico'\n",
    "df_emps_join_estabs = df_emps.join(other=df_estabs, on='cnpj_basico', how='inner')\n",
    "\n",
    "df_emps_join_estabs.show(5)\n",
    "print(df_emps_join_estabs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conta o 'cnpj_basico' por 'ano_de_entrada_sociedade' a partir de 2010\n",
    "# obs.: o método 'sort()' ordenada mais rápido que o 'orderBy()', porém ele pode ordernar errado\n",
    "df_count_por_ano = \\\n",
    "    df_socios\\\n",
    "        .select(\n",
    "            fx.year(df_socios.data_de_entrada_sociedade).alias('ano_de_entrada_sociedade'), \n",
    "            df_socios.cnpj_basico\n",
    "        )\\\n",
    "        .filter('ano_de_entrada_sociedade >= 2010')\\\n",
    "        .groupBy('ano_de_entrada_sociedade')\\\n",
    "        .agg(fx.count(df_socios.cnpj_basico).alias('contagem'))\\\n",
    "        .sort('ano_de_entrada_sociedade')\n",
    "        \n",
    "df_count_por_ano.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dbf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faz a soma da coluna contagem e cria um valor literal para colocar na coluna 'ano_de_entrada_sociedade'\n",
    "df_count_por_ano\\\n",
    "    .select(\n",
    "        fx.lit('Total').alias('ano_de_entrada_sociedade'), # cria um valor literal\n",
    "        fx.sum(df_count_por_ano.contagem).alias('contagem')\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# une o DataFrame com a contagem por ano com a seleção que faz a contagem total + valor literal\n",
    "df_count_por_ano\\\n",
    "    .union(\n",
    "        df_count_por_ano\\\n",
    "        .select(\n",
    "            fx.lit('Total').alias('ano_de_entrada_sociedade'), \n",
    "            fx.sum(df_count_por_ano.contagem).alias('contagem')\n",
    "        )\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac91439",
   "metadata": {},
   "source": [
    "### Exercício 1 - Juntar DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb767bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "produtos = spark.createDataFrame(\n",
    "    [\n",
    "        ('1', 'Bebidas', 'Água mineral'), \n",
    "        ('2', 'Limpeza', 'Sabão em pó'), \n",
    "        ('3', 'Frios', 'Queijo'), \n",
    "        ('4', 'Bebidas', 'Refrigerante'),\n",
    "        ('5', 'Pet', 'Ração para cães')\n",
    "    ],\n",
    "    ['id', 'cat', 'prod']\n",
    ")\n",
    "\n",
    "impostos = spark.createDataFrame(\n",
    "    [\n",
    "        ('Bebidas', 0.15), \n",
    "        ('Limpeza', 0.05),\n",
    "        ('Frios', 0.065),\n",
    "        ('Carnes', 0.08)\n",
    "    ],\n",
    "    ['cat', 'tax']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "produtos.show()\n",
    "impostos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how = a junção usada por padrão é o inner, mas pode ser um desses: \n",
    "# inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti\n",
    "produtos\\\n",
    "    .join(impostos, 'cat', how='outer')\\\n",
    "    .sort(produtos.id)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c7cc2",
   "metadata": {},
   "source": [
    "### Exercício 2 - Juntar DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idades = spark.createDataFrame(\n",
    "    [\n",
    "        ('CARLOS', 15), \n",
    "        ('IVO', 14), \n",
    "        ('MÁRCIA', 16), \n",
    "        ('LEILA', 17), \n",
    "        ('LETÍCIA', 14)\n",
    "    ],\n",
    "    ['nomes', 'idades']\n",
    ")\n",
    "\n",
    "notas = spark.createDataFrame(\n",
    "    [\n",
    "        ('CARLOS', 10), \n",
    "        ('MÁRCIA', 1), \n",
    "        ('LEILA', 10), \n",
    "        ('BRENO', 7), \n",
    "        ('LETÍCIA', 9)\n",
    "    ],\n",
    "    ['nomes', 'notas']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216820a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idades.show()\n",
    "notas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idades\\\n",
    "    .join(notas, 'nomes', how='left')\\\n",
    "    .sort('nomes')\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
