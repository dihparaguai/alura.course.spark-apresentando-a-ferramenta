{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a417b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# coloca as variáveis de ambiente necessárias para o PySpark no windows\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-8.0.452.9-hotspot\"\n",
    "os.environ['SPARK_HOME'] = r\"C:\\spark\\spark-3.5.6-bin-hadoop3\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af419d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark # busca o PySpark no sistema\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381108af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe que inicia uma sessão do Spark para usar o Spark SQL\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicia uma sessão do Spark, que é o ponto de entrada para usar o Spark SQL\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"Iniciando\").config(\"spark.hadoop.io.native.lib\", \"false\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando tuplas\n",
    "data = [('zeca', '35'), ('eva', '29')]\n",
    "col_names = ['nome', 'idade']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando dicionários\n",
    "data_dict = [{'nome': 'zeca', 'idade': '35'}, {'nome': 'eva', 'idade': '29'}]\n",
    "\n",
    "df = spark.createDataFrame(data_dict)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d643b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = os.path.abspath(r'.\\zip_files')\n",
    "data_path = os.path.abspath(r'.\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c89840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile # biblioteca nativa do Python para manipular arquivos zip\n",
    "\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'empresas.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'estabelecimentos.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'socios.zip'), 'r').extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna uma lista com os caminhos dos arquivos de um diretório\n",
    "def get_files_from_directory(directory):\n",
    "    files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "    ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lê todos os arquivos CSV do diretório especificado e cria um DataFrame\n",
    "df_emps = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'empresas')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_emps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'estabelecimentos')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_estabs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc214fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'socios')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_socios.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames\n",
    "df_emps.show(3)\n",
    "df_estabs.show(3)\n",
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para um DataFrame do Pandas\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define os nomes das colunas para os DataFrames\n",
    "\n",
    "emps_col_names = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']\n",
    "\n",
    "estabs_col_names = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']\n",
    "\n",
    "socios_col_names = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41561930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe os nomes das colunas dos DataFrames em formato enumerado para mapeamento das colunas quando renomear\n",
    "for i, col_name in enumerate(emps_col_names):\n",
    "    print([i, col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para renomear as colunas de um DataFrame utilizando o mapeamento de nomes de colunas\n",
    "def rename_columns(df, col_names):\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        df = df.withColumnRenamed(f'_c{i}', col_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renomeia as colunas dos DataFrames\n",
    "df_emps = rename_columns(df_emps, emps_col_names)\n",
    "df_estabs = rename_columns(df_estabs, estabs_col_names)\n",
    "df_socios = rename_columns(df_socios, socios_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames com os nomes das colunas renomeados\n",
    "df_emps.show(3)\n",
    "df_estabs.show(3)\n",
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9656cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para DataFrame do Pandas com o nomes das colunas renomeados\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostra os nomes das colunas e seus tipos de dados\n",
    "df_emps.printSchema()\n",
    "df_socios.printSchema()\n",
    "df_estabs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bba774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe DoubleType para converter o tipo de dado para Double\n",
    "# importa a classe functions do para manipulação de colunas\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as fx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ec566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitui a vírgula por ponto na coluna 'capital_social_da_empresa' para que o PySpark reconheça como número decimal\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', fx.regexp_replace('capital_social_da_empresa', ',', '.'))\n",
    "df_emps.show(3)\n",
    "df_emps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte a coluna 'capital_social_da_empresa' para o tipo Double\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', df_emps['capital_social_da_empresa'].cast(DoubleType()))\n",
    "df_emps.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
