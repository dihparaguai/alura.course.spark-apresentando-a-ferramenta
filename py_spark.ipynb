{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4092b8d",
   "metadata": {},
   "source": [
    "## Importações e inicialização do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a417b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# coloca as variáveis de ambiente necessárias para o PySpark no windows\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-8.0.452.9-hotspot\"\n",
    "os.environ['SPARK_HOME'] = r\"C:\\spark\\spark-3.5.6-bin-hadoop3\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\diego\\Desktop\\spark_alura\\.venv\\Scripts\\python.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af419d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark # busca o PySpark no sistema\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381108af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe que inicia uma sessão do Spark para usar o Spark SQL\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicia uma sessão do Spark, que é o ponto de entrada para usar o Spark SQL\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"Iniciando\").config(\"spark.hadoop.io.native.lib\", \"false\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d7593",
   "metadata": {},
   "source": [
    "## Exemplos de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando tuplas\n",
    "data = [('zeca', '35'), ('eva', '29')]\n",
    "col_names = ['nome', 'idade']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo de criação de um DataFrame usando dicionários\n",
    "data_dict = [{'nome': 'zeca', 'idade': '35'}, {'nome': 'eva', 'idade': '29'}]\n",
    "\n",
    "df = spark.createDataFrame(data_dict)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d643b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte o DataFrame do Spark para um DataFrame do Pandas\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6664f8",
   "metadata": {},
   "source": [
    "## Extrair conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = os.path.abspath(r'.\\zip_files')\n",
    "data_path = os.path.abspath(r'.\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c89840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile # biblioteca nativa do Python para manipular arquivos zip\n",
    "\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'empresas.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'estabelecimentos.zip'), 'r').extractall(data_path)\n",
    "zipfile.ZipFile(os.path.join(zip_path, 'socios.zip'), 'r').extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna uma lista com os caminhos dos arquivos de um diretório\n",
    "def get_files_from_directory(directory):\n",
    "    files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "    ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lê todos os arquivos CSV do diretório especificado e cria um DataFrame\n",
    "df_emps = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'empresas')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_emps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'estabelecimentos')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_estabs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc214fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios = spark.read.csv(\n",
    "    get_files_from_directory(os.path.join(data_path, 'socios')), \n",
    "    sep=\";\", header=False, inferSchema=True)\n",
    "\n",
    "df_socios.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames\n",
    "df_emps.show(3)\n",
    "df_estabs.show(3)\n",
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para um DataFrame do Pandas\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362516c",
   "metadata": {},
   "source": [
    "## Renomear colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define os nomes das colunas para os DataFrames\n",
    "\n",
    "emps_col_names = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa', 'ente_federativo_responsavel']\n",
    "\n",
    "estabs_col_names = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral', 'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria', 'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax', 'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']\n",
    "\n",
    "socios_col_names = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais', 'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41561930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe os nomes das colunas dos DataFrames em formato enumerado para mapeamento das colunas quando renomear\n",
    "for i, col_name in enumerate(emps_col_names):\n",
    "    print([i, col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para renomear as colunas de um DataFrame utilizando o mapeamento de nomes de colunas\n",
    "def rename_columns(df, col_names):\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        df = df.withColumnRenamed(f'_c{i}', col_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renomeia as colunas dos DataFrames\n",
    "df_emps = rename_columns(df_emps, emps_col_names)\n",
    "df_estabs = rename_columns(df_estabs, estabs_col_names)\n",
    "df_socios = rename_columns(df_socios, socios_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exibe as primeiras 3 linhas dos DataFrames com os nomes das colunas renomeados\n",
    "print(df_emps.columns)\n",
    "print(df_estabs.columns)\n",
    "print(df_socios.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9656cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as primeiras 3 linhas do DataFrame do Spark para DataFrame do Pandas com o nomes das colunas renomeados\n",
    "df_socios.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6dafa2",
   "metadata": {},
   "source": [
    "# Converter para coluna de dinheiro para tipo Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostra os nomes das colunas e seus tipos de dados\n",
    "df_emps.printSchema()\n",
    "df_socios.printSchema()\n",
    "df_estabs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bba774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe DoubleType para converter o tipo de dado para Double\n",
    "# importa a classe functions do para manipulação de colunas\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as fx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emps.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ec566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitui a vírgula por ponto na coluna 'capital_social_da_empresa' para que o PySpark reconheça como número decimal\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', fx.regexp_replace('capital_social_da_empresa', ',', '.'))\n",
    "df_emps.select('capital_social_da_empresa').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e18392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte a coluna 'capital_social_da_empresa' para o tipo Double\n",
    "df_emps = df_emps.withColumn('capital_social_da_empresa', df_emps['capital_social_da_empresa'].cast(DoubleType()))\n",
    "df_emps.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36794aef",
   "metadata": {},
   "source": [
    "# Converter coluna para o tipo Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa a classe StringType para converter o tipo de dado para String\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a coluna estava com tipo de dado Integer, mas foi convertida para String para que seja possivel passar to_date\n",
    "df_socios = df_socios.withColumn('data_de_entrada_sociedade', df_socios['data_de_entrada_sociedade'].cast(StringType()))\n",
    "df_socios.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios.select('data_de_entrada_sociedade').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para converter para o tipo Date é necessario converter para String primeiro, depois converte para Date usando o mesmo formato de data na coluna original\n",
    "df_socios = df_socios.withColumn('data_de_entrada_sociedade', fx.to_date(df_socios['data_de_entrada_sociedade'], 'yyyyMMdd'))\n",
    "\n",
    "df_socios.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209df80a",
   "metadata": {},
   "source": [
    "# Converter multiplas colunas em tipo Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8097c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1728a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplica a conversão de data para várias colunas do DataFrame de estabelecimentos\n",
    "df_estabs = df_estabs\\\n",
    "    .withColumn('data_situacao_cadastral', fx.to_date(df_estabs.data_situacao_cadastral\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\\\n",
    "    .withColumn('data_de_inicio_atividade', fx.to_date(df_estabs.data_de_inicio_atividade\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\\\n",
    "    .withColumn('data_da_situacao_especial', fx.to_date(df_estabs.data_da_situacao_especial\\\n",
    "        .cast(StringType()), 'yyyyMMdd'))\n",
    "\n",
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabs.select('data_situacao_cadastral', 'data_de_inicio_atividade', 'data_da_situacao_especial').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18e0fd",
   "metadata": {},
   "source": [
    "## Extrair ano de uma coluna de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assim como no SQL, podemos aplicar funções de transformação nas colunas\n",
    "df_socios\\\n",
    "    .select(\n",
    "        'nome_do_socio_ou_razao_social',\n",
    "        fx.year('data_de_entrada_sociedade').alias('ano_entrada_sociedade'), # extrai o ano da data e renomeia a coluna\n",
    "    )\\\n",
    "    .show(5, truncate=False) # 'truncate=False' para mostrar o texto completo das colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cf5c9",
   "metadata": {},
   "source": [
    "### Exercício - Extrair parte de string de uma coluna:\n",
    "- Criar coluna com ultimo sobrenome e primeiro nome, separados por virgula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d21b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('GISELLE PAULA GUIMARAES CASTRO', 15),\n",
    "    ('ELAINE GARCIA DE OLIVEIRA', 22),\n",
    "    ('JOAO CARLOS ABNER DE LOURDES', 43),\n",
    "    ('MARTA ZELI FERREIRA', 24),\n",
    "    ('LAUDENETE WIGGERS ROEDER', 51)\n",
    "]\n",
    "col_names = ['nome', 'idade']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma coluna com o último sobrenome e o primeiro nome, separados por vírgula\n",
    "df\\\n",
    "    .select(\n",
    "        'nome',\n",
    "        fx.concat_ws( # concatena as colunas com um separador\n",
    "            ', ', \n",
    "            fx.substring_index('nome', ' ', -1), # pega o ultimo sobrenome\n",
    "            fx.substring_index('nome', ' ', 1) # pega o primeiro nome\n",
    "        ).alias('ident'), \n",
    "        'idade'\n",
    "    )\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10164a9a",
   "metadata": {},
   "source": [
    "## Diferença entre NaN, None, Null para o Pandas e o PySpark\n",
    "- para o PySpark, NaN aparece apenas para o tipo Float, Null aparece para todos os tipos de dados\n",
    "- para o Pandas, NaN aparece para todos os tipos de dados exceto para o tipo String, que aparece como None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (float('nan'), None, None, None),\n",
    "    (float('nan'), None, None, None),\n",
    "    (float('nan'), None, None, None),\n",
    "]\n",
    "\n",
    "# determina o nome e o tipo de cada coluna do DataFrame\n",
    "schema = \\\n",
    "    StructType([\n",
    "        StructField(name=\"float_nan\", dataType=FloatType(), nullable=True),\n",
    "        StructField(name=\"float_none\", dataType=FloatType(), nullable=True),\n",
    "        StructField(name=\"integer_none\", dataType=IntegerType(), nullable=True),\n",
    "        StructField(name=\"string_none\", dataType=StringType(), nullable=True),\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "# para o PySpark, NaN aparece apenas para o tipo Float, Null aparece para todos os tipos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()\n",
    "\n",
    "# para o Pandas, NaN aparece para todos os tipos de dados exceto para o tipo String, que aparece como None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d24f1",
   "metadata": {},
   "source": [
    "## Substituir valores Null de tipos diferentes\n",
    "- ao preencher os nulls com 0 (int), o PySpark troca apenas os valores da coluna com tipo numérico\n",
    "- ao preencher os nulls com '0' (str), o PySpark troca apenas os valores da coluna com tipo String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as colunas 'pais' e 'nome_do_representante' possui valores Null e são de tipos diferentes\n",
    "\n",
    "df_socios.select('pais', 'nome_do_representante').show(5, truncate=False)\n",
    "\n",
    "df_socios.select('pais', 'nome_do_representante').printSchema()\n",
    "\n",
    "df_socios.select('pais', 'nome_do_representante').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contagem de valores Null para colunas específicas\n",
    "df_socios\\\n",
    "    .select(\n",
    "    \n",
    "    fx.count( # conta o número de linhas quando a condição de ser Null é atendida\n",
    "        fx.when(\n",
    "            fx.isnull(col='pais'), 1\n",
    "        )\n",
    "    ).alias('pais_null_count'),\n",
    "    \n",
    "    fx.count(\n",
    "        fx.when(\n",
    "            fx.isnull(col='nome_do_representante'), 1\n",
    "        )\n",
    "    ).alias('nome_do_representante_null_count')\n",
    "    \n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562738fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contagem de valores Null em todas as colunas do DataFrame usando lista\n",
    "df_socios\\\n",
    "    .select(\n",
    "    [\n",
    "        fx.count( # conta o número de linhas quando a condição de ser Null é atendida para cada coluna\n",
    "            fx.when(\n",
    "                fx.isnull(col_name), 1\n",
    "            )\n",
    "        )\\\n",
    "        .alias(col_name[:10]) \n",
    "        \n",
    "        for col_name in df_socios.columns\n",
    "    ]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ao preencher os nulls com 0 (int), o PySpark troca apenas os valores da coluna com tipo numérico\n",
    "df_socios.na.fill(0).select('pais', 'nome_do_representante').show(5)\n",
    "\n",
    "# ao preencher os nulls com '0' (str), o PySpark troca apenas os valores da coluna com tipo String\n",
    "df_socios.na.fill('0').select('pais', 'nome_do_representante').show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4cd82",
   "metadata": {},
   "source": [
    "## Ordenar colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios\\\n",
    "    .select(\n",
    "        'nome_do_socio_ou_razao_social', 'data_de_entrada_sociedade'\n",
    "    )\\\n",
    "    .orderBy(\n",
    "        'data_de_entrada_sociedade', ascending=True\n",
    "    )\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atribui uma lista de colunas para o método orderBy, permitindo ordenar por múltiplas colunas\n",
    "df_socios\\\n",
    "    .select(\n",
    "        'nome_do_socio_ou_razao_social', 'data_de_entrada_sociedade'\n",
    "    )\\\n",
    "    .orderBy(\n",
    "        ['data_de_entrada_sociedade', 'nome_do_socio_ou_razao_social'], \n",
    "        ascending=[True, True]\n",
    "    )\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22535c86",
   "metadata": {},
   "source": [
    "### Exercício - Ordenar colunas:\n",
    "- Ordenar o ano e mês dos alunos mais novos para os mais velhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('CARMINA RABELO', 4, 2010), \n",
    "    ('HERONDINA PEREIRA', 6, 2009), \n",
    "    ('IRANI DOS SANTOS', 12, 2010), \n",
    "    ('JOAO BOSCO DA FONSECA', 3, 2009), \n",
    "    ('CARLITO SOUZA', 1, 2010), \n",
    "    ('WALTER DIAS', 9, 2009), \n",
    "    ('BRENO VENTUROSO', 1, 2009), \n",
    "    ('ADELINA TEIXEIRA', 5, 2009), \n",
    "    ('ELIO SILVA', 7, 2010), \n",
    "    ('DENIS FONSECA', 6, 2010)\n",
    "]\n",
    "col_names = ['nome', 'mes', 'ano']\n",
    "\n",
    "df = spark.createDataFrame(data, col_names)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordena por ano e mês, ambos em ordem decrescente, para que os meses mais recentes apareçam primeiro\n",
    "df\\\n",
    "    .orderBy(['ano', 'mes'], ascending=[False, False])\\\n",
    "    .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
